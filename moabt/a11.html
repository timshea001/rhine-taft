<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Statistical Deviations in Card Selection Tests</title>
  <link rel="stylesheet" href="../assets/archive.css" />
</head>
<body class="page page-article">
  <header class="site-header">
    <div class="wrap site-header__inner">
      <div class="article-header">
        <a class="breadcrumb" href="../index.html">Back to archive</a>
        <h1>Statistical Deviations in Card Selection Tests</h1>
        <p class="article-summary">Bridge memo tying earlier card statistics to the reading trials.</p>
      </div>
    </div>
  </header>

  <main class="article-shell wrap">
    <article class="article-body prose">
      <p>
        This file links older card work to the newer text plan. It shows how a bin with five signs can make fast tests, yet how that same bin can miss the shade of a line in open text.
      </p>
      <p>
        A short math note marks how base rate and luck can swing the graph on small runs. The tone is even and keeps to clear terms any lab can check.
      </p>
      <p>
        Card tests have been tried for years. Many labs rely on them. The method is simple: a reader picks one of five cards.
        The data is clean and easy to log during a study. Staff can run many tests in a short time.
        Such speed gives real gains. The plan was for card tests be used for quick scans.
        A varied method, like open text, takes more time. On some days card tests were used as the gold standard.
        That view held for a long time, and it proved to work well in many cases.
      </p>
      <p>
        Yet cards have limits. Many readers felt boxed in by the five choices. They said that their sense of a link was richer than a simple pick.
        A card test can only ask: which one? It cannot ask: what did you feel? Such a gap became clear during the work.
        Staff saw that cards gave clean data but missed the full story. The gains of cards were real, but so were the costs.
        The plan called for labs to think about what they want to know. A varied tool might be better for a varied question.
        On rare days a hybrid method was used, with cards and text side by side. That step was tried but did not work well. The two methods did not mesh well.
      </p>
      <p>
        The math of card tests is clear. Many runs give a base rate of one in five, or twenty percent. Luck can swing the graph on small runs.
        A reader might hit three in a row by chance. Such a streak looks good but means little during a long study.
        Staff had to log many runs to see if the rate rose above chance. The gains of a clear base rate were real.
        The plan called for all card tests use at least a hundred runs. A varied sample size might not show the true rate.
        On some days a small run was tried to save time. That step did not work well. The data was too noisy.
      </p>
      <p>
        Text tests have a varied math. Many words can match or not match. There is no fixed base rate like with cards.
        A reader might write any line at all. Staff had to judge if two lines matched, and that judgment was not always clear during the study.
        Such fuzziness made the data harder to read. The gains of open text were real, but the analysis was tough.
        The plan called for staff to use clear rules for what counts as a match. A varied approach, with loose rules, gave weak data.
        On rare days a strict rule was used, like word-for-word match only. That step worked well in some cases but felt too narrow in others.
      </p>
      <p>
        Luck plays a role in both methods. Many runs can show a false trend. A card test might show a high rate for a while, then drop.
        A text test might give matches on a few runs, then go flat during the rest. Staff had to track long spans to see the real trend.
        Such care gave real gains. The plan called for all tests run for months, not days.
        A varied timeline might miss the full picture. On some days a short burst was tried to test a strong reader.
        That step worked well only when the reader had already shown a clear trend in long runs.
      </p>
      <p>
        The file also talks about how to report the data. Many graphs and tables were made. Staff tried to show the trend without hiding the noise.
        A graph that only shows the hits looks good, but it lies. The full data, with hits and misses, tells the real story during the study.
        Such honesty gave real gains. The plan called for all reports show the full spread.
        A varied style might cherry-pick the best runs, but that would not serve the field. On rare days a simple sum was tried for talks or brief notes.
        That method worked well only when the full data was also made public.
      </p>
      <p>
        The file ends with a balanced view. Many tools exist, and each has its place. Card tests are fast, clean, and easy to analyze.
        Text tests are slow, fuzzy, and hard to judge, but they give a richer view during the work. Both methods have real gains.
        The plan called for labs to use both when they can. A varied choice might work for a varied goal.
        On rare days a lab might focus on just one method. That is fine, but know the trade-offs.
        For now, these notes offer a clear look at the math and the limits of each tool. Use them as a guide. Both methods work well when used with care.
      </p>
    </article>
    <p class="article-footnote">Internal memorandum: condensed observations from controlled reading trials.</p>
  </main>
  <script src="../scripts/logic.js"></script>
</body>
</html>
