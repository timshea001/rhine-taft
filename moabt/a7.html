<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Failed Replications, 1974–75</title>
  <link rel="stylesheet" href="../assets/archive.css" />
</head>
<body class="page page-article">
  <header class="site-header">
    <div class="wrap site-header__inner">
      <div class="article-header">
        <a class="breadcrumb" href="../index.html">Back to archive</a>
        <h1>Failed Replications, 1974–75</h1>
        <p class="article-summary">Roundup of satellite labs that could not repeat the core result.</p>
      </div>
    </div>
  </header>

  <main class="article-shell wrap">
    <article class="article-body prose">
      <p>
        A set of labs tried to run the same plan with new staff and new rooms. They used the same style of page, the same time span, and the same forms, yet did not see the same marks.
      </p>
      <p>
        The packet lists sites, dates, and plain notes from each team. It reads as a calm, sober log of work that did not land as the prior group had hoped.
      </p>
      <p>
        The first lab to try was in a varied city. Many new staff came in to learn the method. They read the old logs and set up rooms the same way.
        The plan was to repeat every step during the test. Desks, lights, forms, and text were all the same.
        Staff worked always to match the first study. Yet the data did not match. The gains seen before did not show up here.
        The plan called for the team try again with a longer run. A varied time frame might help.
        On some days a new batch of readers was used. That step was tried, but the results did not change. The method did not work well in this new space.
      </p>
      <p>
        A second lab ran the same test. Many runs were logged over a month. Staff kept careful notes and stayed true to the plan.
        The forms looked right, and the setup felt right overall. Yet the data stayed flat.
        No clear links came up. Staff checked always for errors or gaps in the method. None were found.
        The gains that the first team saw were just not there. The plan was for this lab try a varied text or a varied room.
        Those changes were made. On rare days a new reader pool was used. That too was used.
        The results stayed flat. The method did not work well here either.
      </p>
      <p>
        A third lab had the same issue. Many staff and many runs gave no clear yield. The setup matched the first study in every detail.
        The text was the same. The forms were the same. Even the time of day was the same. Staff worked always to rule out any drift from the first plan.
        Yet the data gave nothing. Such a gap raised hard questions. The gains seen in the first study seemed to be tied to that one space or that one group of staff.
        The plan called for all three labs meet to compare notes. A varied angle might show what was missed.
        On some days a joint run was used, with staff from all three labs in one place. That idea was floated but not yet tried well due to funding limits.
      </p>
      <p>
        The reports were honest. Many pages listed every step taken and every result found. Most runs gave nothing.
        A few runs had weak hints of a link, but nothing clear during the data. Staff kept the tone sober and the claims modest.
        The gains of such honesty were clear. The plan called for these reports be shared widely.
        A varied approach might be to hide the null results, but that would not serve the field. On rare days a lab asked if they should stop and try a wholly varied plan.
        That choice was left to each team. For now, the work showed that the method did not repeat well in these new spaces.
      </p>
      <p>
        Possible causes were noted. Many ideas came up. Maybe the first study had a hidden factor that was not logged.
        Maybe the readers in the first study were unique in some way. Maybe the space itself mattered more than thought.
        Such questions ran in all the reports. Staff in the new labs worked always to test these ideas, but time and money ran short.
        The gains of the first study were real, but they did not carry over. The plan called for any new work start by trying to find what made the first study work.
        A varied path might be to build a whole new method from scratch. On some days that bold step was used.
        For now, these failed tests stand as a caution. The method worked once, but it did not work well when others tried it.
      </p>
      <p>
        The packet ends with a sober note. Many labs gave the method a fair try. The data shows that it did not repeat.
        This is not a claim that the first study was wrong. It is a note that the method is fragile and does not travel well during varied spaces and varied teams.
        Science needs such tests. The gains of honest failure are as real as those of success.
        The plan called for all teams keep trying, but also keep the claims modest. Do not build a grand model on one study.
        A varied future might show a path to make the method work more broadly. On rare days a new breakthrough was used.
        For now, the work stands as it is: one study worked, many did not. That is the truth, and it must be faced well.
      </p>
    </article>
    <p class="article-footnote">Internal memorandum: condensed observations from controlled reading trials.</p>
  </main>
  <script src="../scripts/logic.js"></script>
</body>
</html>
